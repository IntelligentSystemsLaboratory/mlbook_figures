[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLbook Figures",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "chapter_2/covroc_right.html",
    "href": "chapter_2/covroc_right.html",
    "title": "2.5 Covroc right",
    "section": "",
    "text": "In a coverage plot, accuracy isometrics have a slope of 1, and average recall isometrics are parallel to the ascending diagonal. In the corresponding ROC plot, average recall isometrics have a slope of 1; the accuracy isometric here has a slope of 3, corresponding to the ratio of negatives to positives in the data set."
  },
  {
    "objectID": "chapter_2/roc_right.html",
    "href": "chapter_2/roc_right.html",
    "title": "2.3 Roc right",
    "section": "",
    "text": "C1 and C3 both dominate C2, but neither dominates the other. The diagonal line indicates that C1 and C3 achieve equal accuracy.The same plot with normalised axes. We can interpret this plot as a merger of the two coverage plots in coverage, employing normalisation to deal with the different class distributions. The diagonal line now indicates that C1 and C3 have the same average recall."
  },
  {
    "objectID": "chapter_2/covroc_left.html",
    "href": "chapter_2/covroc_left.html",
    "title": "2.5 Covroc left",
    "section": "",
    "text": "In a coverage plot, accuracy isometrics have a slope of 1, and average recall isometrics are parallel to the ascending diagonal. In the corresponding ROC plot, average recall isometrics have a slope of 1; the accuracy isometric here has a slope of 3, corresponding to the ratio of negatives to positives in the data set."
  },
  {
    "objectID": "chapter_2/coverage_right.html",
    "href": "chapter_2/coverage_right.html",
    "title": "2.2 Coverage Right",
    "section": "",
    "text": "A coverage plot depicting the two contingency tables in Table 2.2. The plot is square because the class distribution is uniform (right). Coverage plot for Example 2.1, with a class ratio clr=3."
  },
  {
    "objectID": "chapter_2/roc_left.html",
    "href": "chapter_2/roc_left.html",
    "title": "2.3 Roc left",
    "section": "",
    "text": "C1 and C3 both dominate C2, but neither dominates the other. The diagonal line indicates that C1 and C3 achieve equal accuracy.The same plot with normalised axes. We can interpret this plot as a merger of the two coverage plots in coverage, employing normalisation to deal with the different class distributions. The diagonal line now indicates that C1 and C3 have the same average recall."
  },
  {
    "objectID": "chapter_2/coverage_left.html",
    "href": "chapter_2/coverage_left.html",
    "title": "2.2 Coverage Left",
    "section": "",
    "text": "A coverage plot depicting the two contingency tables in Table 2.2. The plot is square because the class distribution is uniform (right). Coverage plot for Example 2.1, with a class ratio clr=3."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "chapter_7/linkernel.html",
    "href": "chapter_7/linkernel.html",
    "title": "Lin Kernel",
    "section": "",
    "text": "&lt;module 'matplotlib.pyplot' from '/home/ck22122/.local/share/virtualenvs/superbook2-Q63KuxOb/lib/python3.10/site-packages/matplotlib/pyplot.py'&gt;"
  },
  {
    "objectID": "chapter_8/twonex.html",
    "href": "chapter_8/twonex.html",
    "title": "Twonex",
    "section": "",
    "text": "/tmp/ipykernel_310223/3250587599.py:46: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  scatter = ax.scatter(Te[:, 0], Te[:, 1], c=Lb, s=3, cmap=plt.cm.get_cmap('tab10'))\n/tmp/ipykernel_310223/3250587599.py:46: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  scatter = ax.scatter(Te[:, 0], Te[:, 1], c=Lb, s=3, cmap=plt.cm.get_cmap('tab10'))\n/tmp/ipykernel_310223/3250587599.py:46: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  scatter = ax.scatter(Te[:, 0], Te[:, 1], c=Lb, s=3, cmap=plt.cm.get_cmap('tab10'))"
  },
  {
    "objectID": "chapter_8/mlmethods2-complete.html",
    "href": "chapter_8/mlmethods2-complete.html",
    "title": "ML Methods",
    "section": "",
    "text": "k = 2, Sum of squared distances (inertia): 560.625\nCluster 1:\n['naive Bayes' 'kNN' 'Linear Classifier' 'Linear Regression'\n 'Logistic Regression' 'SVM' 'Kmeans' 'GMM']\nCluster 2:\n['Trees' 'Rules' 'Associations']\nk = 3, Sum of squared distances (inertia): 293.5\nCluster 1:\n['naive Bayes' 'GMM']\nCluster 2:\n['Trees' 'Rules' 'Associations']\nCluster 3:\n['kNN' 'Linear Classifier' 'Linear Regression' 'Logistic Regression' 'SVM'\n 'Kmeans']\nk = 4, Sum of squared distances (inertia): 207.5\nCluster 1:\n['kNN' 'Linear Classifier' 'Linear Regression' 'Logistic Regression' 'SVM'\n 'Kmeans']\nCluster 2:\n['naive Bayes']\nCluster 3:\n['Trees' 'Rules' 'Associations']\nCluster 4:\n['GMM']\nk = 5, Sum of squared distances (inertia): 148.0\nCluster 1:\n['kNN' 'Linear Regression' 'Logistic Regression' 'SVM' 'Kmeans']\nCluster 2:\n['naive Bayes']\nCluster 3:\n['Trees' 'Rules' 'Associations']\nCluster 4:\n['GMM']\nCluster 5:\n['Linear Classifier']"
  },
  {
    "objectID": "chapter_8/randomdendrogram.html",
    "href": "chapter_8/randomdendrogram.html",
    "title": "Random Dendogram",
    "section": "",
    "text": "Spearman correlation for complete: 0.8083057874466868"
  },
  {
    "objectID": "chapter_9/linclass2.html",
    "href": "chapter_9/linclass2.html",
    "title": "Linear Classifier 2",
    "section": "",
    "text": "Error for BLC: 8\nError for LSC: 11\nOptimization terminated successfully.\n         Current function value: 0.112231\n         Iterations 9\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  200\nModel:                          Logit   Df Residuals:                      197\nMethod:                           MLE   Df Model:                            2\nDate:                Thu, 27 Feb 2025   Pseudo R-squ.:                  0.8381\nTime:                        15:31:02   Log-Likelihood:                -22.446\nconverged:                       True   LL-Null:                       -138.63\nCovariance Type:            nonrobust   LLR p-value:                 3.486e-51\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.6145      0.682      0.901      0.368      -0.723       1.952\nx1             1.8406      0.440      4.179      0.000       0.977       2.704\nx2             3.9016      0.888      4.395      0.000       2.162       5.642\n==============================================================================\nError for Logistic Regression: 12\nBinary Logistic Regression Error: 0.036650428051045035"
  },
  {
    "objectID": "chapter_9/logreg.html",
    "href": "chapter_9/logreg.html",
    "title": "Logreg",
    "section": "",
    "text": "Error Rate Logistic Calibration: 0.12021634021426919\nError Rate Isotonic Calibration: 0.0811965811965812\nError Rate Logistic Regression: 0.11949469490221816"
  },
  {
    "objectID": "chapter_9/gmm.html",
    "href": "chapter_9/gmm.html",
    "title": "GMM",
    "section": "",
    "text": "Final means: [53 64]\nFinal variances: [ 2 30]\nInitial means: [53 64]"
  },
  {
    "objectID": "chapter_10/logcal.html",
    "href": "chapter_10/logcal.html",
    "title": "Logcal",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef linclass(mupos=None, sigpos=None, rhopos=None, muneg=None, signeg=None, rhoneg=None):\n    # Default settings if parameters are not provided\n    if mupos is None:\n        mupos = np.array([1, 1])\n        sigpos = np.array([0.4, 0.4])\n        rhopos = 0.2\n        muneg = np.array([-1, -1])\n        signeg = np.array([0.4, 0.4])\n        rhoneg = -0.2\n\n    # Covariance matrices\n    covpos = rhopos * np.sqrt(sigpos[0] * sigpos[1])\n    sigmapos = np.array([[sigpos[0], covpos], [covpos, sigpos[1]]])\n    \n    covneg = rhoneg * np.sqrt(signeg[0] * signeg[1])\n    sigmaneg = np.array([[signeg[0], covneg], [covneg, signeg[1]]])\n\n    Npos = 100\n    Nneg = 50\n\n    # Generate data using multivariate normal distribution\n    pos = np.random.multivariate_normal(mupos, sigmapos, Npos)\n    neg = np.random.multivariate_normal(muneg, sigmaneg, Nneg)\n\n    # Do the plotting\n    plt.figure(figsize=(8, 8))\n    plt.xlim([-3, 3])\n    plt.ylim([-3, 3])\n    plt.scatter(pos[:, 0], pos[:, 1], color='k', marker='+')\n    plt.scatter(neg[:, 0], neg[:, 1], color='k', marker='.')\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Homogeneous coordinates\n    pos1 = np.hstack([np.ones((Npos, 1)), pos])\n    neg1 = np.hstack([np.ones((Nneg, 1)), neg])\n\n    # Basic linear classifier with homogeneous coordinates\n    emupos1 = np.mean(pos1, axis=0)\n    emuneg1 = np.mean(neg1, axis=0)\n    \n    # Plot means of the positive and negative samples\n    plt.plot(emupos1[1], emupos1[2], 'ro')\n    plt.plot(emuneg1[1], emuneg1[2], 'ro')\n\n    # Plot line between the two means\n    plt.plot([emupos1[1], emuneg1[1]], [emupos1[2], emuneg1[2]], 'r:', label='Separation Line')\n\n    # Line representing the decision boundary of the linear classifier\n    blc = emupos1 - emuneg1\n    x_vals = np.array([-2, 2])\n    y_vals = (-blc[0] + x_vals * 2 * blc[1]) / blc[2]\n    plt.plot(x_vals, y_vals, 'r-', label='Decision Boundary')\n\n    # Show plot with legend\n    plt.legend()\n    plt.show()\n\n# Example usage with default parameters\nlinclass()"
  }
]